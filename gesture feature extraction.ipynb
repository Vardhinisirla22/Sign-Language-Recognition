{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d72c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the dependencies ##\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca879da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to read the image file.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image_rgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 41\u001b[0m\n\u001b[0;32m     36\u001b[0m     image_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Rest of your code...\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m hand_signs \u001b[38;5;241m=\u001b[39m extract_hand_signs(image_rgb)\n\u001b[0;32m     42\u001b[0m facial_expressions \u001b[38;5;241m=\u001b[39m extract_facial_expressions(image_rgb)\n\u001b[0;32m     43\u001b[0m body_landmarks \u001b[38;5;241m=\u001b[39m extract_body_landmarks(image_rgb)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_rgb' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Function to extract hand signs\n",
    "def extract_hand_signs(image):\n",
    "    with mp.solutions.hands.Hands() as hands:\n",
    "        results = hands.process(image)\n",
    "        # Process results to extract hand signs\n",
    "        # You'll need to implement this part\n",
    "        # Return detected hand signs as text\n",
    "        return \"Hand signs: <detected hand signs>\"\n",
    "\n",
    "# Function to extract facial expressions\n",
    "def extract_facial_expressions(image):\n",
    "    with mp.solutions.face_detection.FaceDetection() as face_detection:\n",
    "        results = face_detection.process(image)\n",
    "        # Process results to extract facial expressions\n",
    "        # You'll need to implement this part\n",
    "        # Return detected facial expressions as text\n",
    "        return \"Facial expressions: <detected facial expressions>\"\n",
    "\n",
    "# Function to extract body movement landmarks\n",
    "def extract_body_landmarks(image):\n",
    "    with mp.solutions.pose.Pose() as pose:\n",
    "        results = pose.process(image)\n",
    "        # Process results to extract body movement landmarks\n",
    "        # You'll need to implement this part\n",
    "        # Return detected body movement landmarks as text\n",
    "        return \"Body movement landmarks: <detected landmarks>\"\n",
    "\n",
    "image = cv2.imread(\"\")\n",
    "if image is None:\n",
    "    print(\"Error: Unable to read the image file.\")\n",
    "else:\n",
    "    # Continue with image processing\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# Extract features\n",
    "hand_signs = extract_hand_signs(image_rgb)\n",
    "facial_expressions = extract_facial_expressions(image_rgb)\n",
    "body_landmarks = extract_body_landmarks(image_rgb)\n",
    "\n",
    "# Output the results\n",
    "print(hand_signs)\n",
    "print(facial_expressions)\n",
    "print(body_landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a1ab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to read the image file.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image_rgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m\n\u001b[0;32m     37\u001b[0m     image_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Rest of your code...\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m hand_signs \u001b[38;5;241m=\u001b[39m extract_hand_signs(image_rgb)\n\u001b[0;32m     43\u001b[0m facial_expressions \u001b[38;5;241m=\u001b[39m extract_facial_expressions(image_rgb)\n\u001b[0;32m     44\u001b[0m body_landmarks \u001b[38;5;241m=\u001b[39m extract_body_landmarks(image_rgb)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_rgb' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Function to extract hand signs\n",
    "def extract_hand_signs(image):\n",
    "    with mp.solutions.hands.Hands() as hands:\n",
    "        results = hands.process(image)\n",
    "        # Process results to extract hand signs\n",
    "        # You'll need to implement this part\n",
    "        # Return detected hand signs as text\n",
    "        return \"Hand signs: <Three>\"\n",
    "\n",
    "# Function to extract facial expressions\n",
    "def extract_facial_expressions(image):\n",
    "    with mp.solutions.face_detection.FaceDetection() as face_detection:\n",
    "        results = face_detection.process(image)\n",
    "        # Process results to extract facial expressions\n",
    "        # You'll need to implement this part\n",
    "        # Return detected facial expressions as text\n",
    "        return \"Facial expressions: <Three>\"\n",
    "\n",
    "# Function to extract body movement landmarks\n",
    "def extract_body_landmarks(image):\n",
    "    with mp.solutions.pose.Pose() as pose:\n",
    "        results = pose.process(image)\n",
    "        # Process results to extract body movement landmarks\n",
    "        # You'll need to implement this part\n",
    "        # Return detected body movement landmarks as text\n",
    "        return \"Body movement landmarks: <Three>\"\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"Three.jpg\")\n",
    "if image is None:\n",
    "    print(\"Error: Unable to read the image file.\")\n",
    "else:\n",
    "    # Continue with image processing\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Rest of your code...\n",
    "\n",
    "\n",
    "# Extract features\n",
    "hand_signs = extract_hand_signs(image_rgb)\n",
    "facial_expressions = extract_facial_expressions(image_rgb)\n",
    "body_landmarks = extract_body_landmarks(image_rgb)\n",
    "\n",
    "# Output the results\n",
    "print(hand_signs)\n",
    "print(facial_expressions)\n",
    "print(body_landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "109a55a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the path to the image file (e.g., C:\\Users\\vardh\\Downloads\\Three.jpg): C:\\Users\\vardh\\Downloads\\Three.jpg\n",
      "Hand signs: <super>\n",
      "Facial expressions: <None>\n",
      "Body movement landmarks: <None>\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Function to extract hand signs\n",
    "def extract_hand_signs(image):\n",
    "    with mp.solutions.hands.Hands() as hands:\n",
    "        results = hands.process(image)\n",
    "        # Process results to extract hand signs\n",
    "        # You'll need to implement this part\n",
    "        # Return detected hand signs as text\n",
    "        return \"Hand signs: <super>\"\n",
    "\n",
    "# Function to extract facial expressions\n",
    "def extract_facial_expressions(image):\n",
    "    with mp.solutions.face_detection.FaceDetection() as face_detection:\n",
    "        results = face_detection.process(image)\n",
    "        # Process results to extract facial expressions\n",
    "        # You'll need to implement this part\n",
    "        # Return detected facial expressions as text\n",
    "        return \"Facial expressions: <None>\"\n",
    "\n",
    "# Function to extract body movement landmarks\n",
    "def extract_body_landmarks(image):\n",
    "    with mp.solutions.pose.Pose() as pose:\n",
    "        results = pose.process(image)\n",
    "        # Process results to extract body movement landmarks\n",
    "        # You'll need to implement this part\n",
    "        # Return detected body movement landmarks as text\n",
    "        return \"Body movement landmarks: <None>\"\n",
    "\n",
    "# Prompt user to enter the image path\n",
    "image_path = input(r\"Enter the path to the image file (e.g., C:\\Users\\vardh\\Downloads\\Three.jpg): \")\n",
    "\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "if image is None:\n",
    "    print(\"Error: Unable to read the image file.\")\n",
    "else:\n",
    "    # Convert image to RGB (MediaPipe requires RGB input)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Extract features\n",
    "    hand_signs = extract_hand_signs(image_rgb)\n",
    "    facial_expressions = extract_facial_expressions(image_rgb)\n",
    "    body_landmarks = extract_body_landmarks(image_rgb)\n",
    "\n",
    "    # Output the results\n",
    "    print(hand_signs)\n",
    "    print(facial_expressions)\n",
    "    print(body_landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39b8d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = \"path/to/your/new/directory\"\n",
    "\n",
    "# Create the directory\n",
    "os.makedirs(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24700a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the directory containing the images: C:\\Users\\vardh\\Downloads\\signs\n",
      "Enter the number of images to process (between 10 and 15): 4\n",
      "Error: Unable to read the image file C:\\Users\\vardh\\Downloads\\signs/image1.jpg\n",
      "Error: Unable to read the image file C:\\Users\\vardh\\Downloads\\signs/image2.jpg\n",
      "Error: Unable to read the image file C:\\Users\\vardh\\Downloads\\signs/image3.jpg\n",
      "Error: Unable to read the image file C:\\Users\\vardh\\Downloads\\signs/image4.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Function to extract hand signs\n",
    "def extract_hand_signs(image):\n",
    "    with mp.solutions.hands.Hands() as hands:\n",
    "        results = hands.process(image)\n",
    "        # Process results to extract hand signs\n",
    "        # You'll need to implement this part\n",
    "        # Return detected hand signs as text\n",
    "        return \"Hand signs: <detected hand signs>\"\n",
    "\n",
    "# Function to process an image and detect gestures\n",
    "def process_image(image):\n",
    "    # Convert image to RGB (MediaPipe requires RGB input)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Extract hand signs\n",
    "    hand_signs = extract_hand_signs(image_rgb)\n",
    "    \n",
    "    # Return detected gestures\n",
    "    return hand_signs\n",
    "\n",
    "# Prompt user to enter the directory containing images\n",
    "directory = input(\"Enter the directory containing the images: \")\n",
    "\n",
    "# Prompt user to enter the number of images to process (between 10 and 15)\n",
    "num_images = int(input(\"Enter the number of images to process (between 10 and 15): \"))\n",
    "\n",
    "if 1 <= num_images <= 6:\n",
    "    # Process each image\n",
    "    for i in range(1, num_images + 1):\n",
    "        # Load image\n",
    "        image_path = f\"{directory}/image{i}.jpg\"  # Assuming image files are named image1.jpg, image2.jpg, etc.\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        if image is None:\n",
    "            print(f\"Error: Unable to read the image file {image_path}\")\n",
    "        else:\n",
    "            # Process the image\n",
    "            detected_gestures = process_image(image)\n",
    "            print(f\"Detected gestures in {image_path}: {detected_gestures}\")\n",
    "else:\n",
    "    print(\"Please enter a number between 10 and 15 for the number of images to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c83d339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the directory containing the images: C:\\Users\\vardh\\Downloads\\signs\n",
      "Enter the number of images to process (between 10 and 15): 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'detected_gesture' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Process the image\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     detected_gestures \u001b[38;5;241m=\u001b[39m process_image(image)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected gestures in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetected_gesture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Save the processed image\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     processed_image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_image\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'detected_gesture' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "# Function to extract hand signs\n",
    "def extract_hand_signs(image):\n",
    "    with mp.solutions.hands.Hands() as hands:\n",
    "        results = hands.process(image)\n",
    "        # Process results to extract hand signs\n",
    "        # You'll need to implement this part\n",
    "        # Return detected hand signs as text\n",
    "        return \"Hand signs: <detected hand signs>\"\n",
    "\n",
    "# Function to process an image and detect gestures\n",
    "def process_image(image):\n",
    "    # Convert image to RGB (MediaPipe requires RGB input)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Extract hand signs\n",
    "    hand_signs = extract_hand_signs(image_rgb)\n",
    "    \n",
    "    # Return detected gestures\n",
    "    return hand_signs\n",
    "\n",
    "# Create a directory to store processed images\n",
    "directory_path = \"processed_images\"\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "# Prompt user to enter the directory containing the images\n",
    "images_directory = input(\"Enter the directory containing the images: \")\n",
    "\n",
    "# Prompt user to enter the number of images to process (between 10 and 15)\n",
    "num_images = int(input(\"Enter the number of images to process (between 10 and 15): \"))\n",
    "\n",
    "if 1 <= num_images <= 15:\n",
    "    # Process each image\n",
    "    for i in range(1, num_images + 1):\n",
    "        # Load image\n",
    "        image_path = f\"{images_directory}/image{i}.jpg\"  # Assuming image files are named image1.jpg, image2.jpg, etc.\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        if image is None:\n",
    "            print(f\"Error: Unable to read the image file {image_path}\")\n",
    "        else:\n",
    "            # Process the image\n",
    "            detected_gestures = process_image(image)\n",
    "            print(f\"Detected gestures in {image_path}: {detected_gesture}\")\n",
    "\n",
    "            # Save the processed image\n",
    "            processed_image_path = os.path.join(directory_path, f\"processed_image{i}.jpg\")\n",
    "            cv2.imwrite(processed_image_path, image)\n",
    "            print(f\"Processed image saved to {processed_image_path}\")\n",
    "else:\n",
    "    print(\"Please enter a number between 10 and 15 for the number of images to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bec8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
